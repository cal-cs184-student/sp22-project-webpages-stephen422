<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>  
    div.padded {  
      padding-top: 0px;  
      padding-right: 100px;  
      padding-bottom: 0.25in;  
      padding-left: 100px;  
    }  
  </style> 
<title>Assignment 3  |  Hansung Kim  |  CS 184</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="style.css" media="screen" />
</head>
<body>
<br />
<h1 align="middle">Assignment 3: PathTracer</h1>
<h2 align="middle">Hansung Kim</h2>
<h2 align="middle">CS284A, Spring 2022</h2>
<h3 align="middle">Repository: <a href="https://github.com/cal-cs184-student/p3-1-pathtracer-sp22-pathgrinder">https://github.com/cal-cs184-student/p3-1-pathtracer-sp22-pathgrinder</a></h3>
<h3 align="middle">Github Page: <a href="https://cal-cs184-student.github.io/sp22-project-webpages-stephen422/">https://cal-cs184-student.github.io/sp22-project-webpages-stephen422/</a></h3>

<div class="padded">

<h2 align="middle">Use of late days</h2>

<p>
I included this part as I submitted this one quite late,
and wanted to clarify my use of slip days!
</p>

<p>
I submitted the submission on 3/31, which counts as 8 late days:
3/16, 17, 18, 19~27 (Spring break as 1 day), 28, 29, 30, 31.
I didn't use any of my 7 slip days on the earlier assignments, so I believe
in total a late penalty of 1 day is applied.
Please let me know if there is any discrepancy with the actual grading!
</p>

<h2 align="middle">Overview</h2>
  <p>
    In this assignment, we implemented a physically-based global illumination
    renderer applying all concepts we learned e.g. BRDFs, Monte Carlo
    estimation, path tracing, direct/indirect illumination, etc.
  </p>

<h2 align="middle">Part 1: Ray Generation and Intersection</h2>
  <h4>
    Walk through the ray generation and primitive intersection parts of the rendering pipeline.
  </h4>
  <p>
    Ray generation is the first step in the rendering pipeline where we generate
    samples of primary rays that are shot from the eye, to a specific point on
    the virtual image.  These rays will be the starting point to the whole paths that
    will be generated according to the global illumination algorithm, implemented in
    the later parts.
    Care must be taken to add a random amount of "jitter" to
    each pixel position samples so that the rays are sampled to be directed to uniformly
    random screen positions and therefore estimate the render more accurately.
  </p>
  <p>
    We also implemented intersection for triangle and sphere, which are crucial
    to find out which part of the scene the casted rays hit.  We need to figure out
    not only the pointer to the hit primitive, but also the specific barycentric
    coordinate of the hit point that resides on the primitive.
  </p>

  <h4>
    Explain the triangle intersection algorithm you implemented in your own words.
  </h4>
  <p>
  I used the Moller-Trumbore algorithm, exactly as described in the
  <a href=https://cs184.eecs.berkeley.edu/sp22/lecture/9-22/ray-tracing>lecture slide</a>.
  Once I compute the <code>b1</code> and <code>b2</code> values, I can use the barycentric
  coordinate interpolation to get the interpolated normal vector of the intersection
  point, which I do in the <code>Triangle::intersect</code> function.
  </p>

  <h4>
    Show images with normal shading for a few small .dae files.
  </h4>
  <p>
    Thanks to the normal interpolation, the images look smooth even with low
    number of primitives.
  </p>

  <div align="center">
    <table style="width=100%">
      <tr>
        <td align="middle">
          <img src="images/teapot-normal.png" width="480px" />
        </td>
        <td align="middle">
          <img src="images/beast-normal.png" width="480px" />
        </td>
      </tr>
      <tr>
        <td align="middle">
          <img src="images/maxplanck-normal.png" width="480px" />
        </td>
        <td align="middle">
          <img src="images/peter-normal.png" width="480px" />
        </td>
      </tr>
    </table>
  </div>


<h2 align="middle">Part 2: Bounding Volume Hierarchy</h2>

<h4>
  Walk through your BVH construction algorithm. Explain the heuristic you chose
  for picking the splitting point.
</h4>
<p>
  I chose the axis along which the bounding box has the longest extent as the
  splitting axis, and chose the median point in terms of number of objects
  as the split point. For each iteration at each tree level, I sort the
  list of objects by the coordinate of their centroids along the split axis
  before I compute the median.
</p>

<h4>
  Show images with normal shading for a few large .dae files that you can only
  render with BVH acceleration.
</h4>
<p>
As shown in Part 1, <code>maxplanck.dae</code> and <code>peter.dae</code> are the ones
that have large enough primitives that it does not really make sense to render
without the BVH acceleration.
</p>
  <div align="center">
    <table style="width=100%">
      <tr>
        <td align="middle">
          <img src="images/maxplanck-normal.png" width="480px" />
        </td>
        <td align="middle">
          <img src="images/peter-normal.png" width="480px" />
        </td>
      </tr>
    </table>
  </div>

<h4>
  Compare rendering times on a few scenes with moderately complex geometries
  with and without BVH acceleration. Present your results in a one-paragraph
  analysis.
</h4>
<p>
<li><code>maxplanck.dae</code> takes 0.0823s with BVH | 106.2s without BVH | 50801 primitives</li>
<li><code>peter.dae</code> takes 0.0942s with BVH | 91.9s without BVH | 40018 primitives</li>
<li><code>beast.dae</code> takes 0.0666s with BVH | 145.2 without BVH | 64618 primitives</li>
</p>
<p>
The reason why the BVH acceleration speed up the time so much is because it
converts the time complexity of the ray casting from <code>O(R*N)</code>
to <code>O(R*logN)</code>, where R is the number of ray and N is the number
of primitives.  Without the BVH, the algorithm iterates over entire list
of primitives, over all rays.  With the BVH, there's still the iteration
over all rays, but the intersection operation for each ray gets cut down to
<code>O(logN)</code> as the time is proportional to the traversed depth of the tree.
This difference in time complexity gets much more dramatic with larger scenes
like the above two.
</p>


<h2 align="middle">Part 3: Direct Illumination</h2>

<h4>
  Walk through both implementations of the direct lighting function.
</h4>
  <p>
  <em>Uniform hemisphere sampling</em>: the implementation starts by having a loop
  that iterates for each new sample.  We first generate a sampled ray using
  <code>hit_p</code> as origin and <code>hemisphereSampler->get_sample()</code>
  as direction, where the latter is in the object space and therefore should
  be converted to world space using <code>o2w</code>.  The ray's
  <code>min_t</code> is also set to EPS_F to avoid floating point problems.
  </p>
  <p>
  We then use <code>zero_bounce_radiance</code> to find out the emission
  along the ray, multiply that with the BSDF and pdf as per the
  rendering equation, add that to a radiance sum variable <code>light_out</code>,
  and finally normalize it by <code>num_samples</code> to make it unbiased.
  </p>
  <p>
  <em>Importance sampling</em>: Same as above, but there's a nested for loop
  where we (1) iterative over all lights, and (2) iterate over light samples
  for each light.  For the inner loop (2), we set the loop counter conditionally
  depending on whether the light is point light, so that we only iterate
  once for the point light.  For point lights, we multiply the radiance value by
  <code>ns_area_light</code> so that its sampling contribution is equal to
  other area lights.  <code>max_t</code> is set to <code>dist_to_light - EPS_F</code>
  to avoid floating point problems.  Lastly, we normalize the radiance sum
  by <code>ns_area_light</code> to make it an unbiased estimator.
  </p>

<h4>
  Show some images rendered with both implementations of the direct lighting
  function.
</h4>
<p>
  Below are renders of CBbunny, dragon and wall-e, rendered with 4 samples per pixel
  and 1 sample per light.  Note that dragon and wall-e is rendered completely black
  for hemisphere sampling, because the probability of a hemisphere sampled
  ray hitting the point light is almost zero.
</p>

<div align="center">
  <table style="width=100%">
    <tr>
      <td align="middle">
        <img src="images/CBbunny-hemisphere-4-1.png" width="480px" />
      </td>
      <td align="middle">
        <img src="images/CBbunny-importance-4-1.png" width="480px" />
      </td>
    </tr>
    <tr>
      <td align="middle">
        <img src="images/dragon-hemisphere-4-1.png" width="480px" />
      </td>
      <td align="middle">
        <img src="images/dragon-importance-4-1.png" width="480px" />
      </td>
    </tr>
    <tr>
      <td align="middle">
        <img src="images/wall-e-hemisphere-4-1.png" width="480px" />
      </td>
      <td align="middle">
        <img src="images/wall-e-importance-4-1.png" width="480px" />
      </td>
    </tr>
  </table>
  <figcaption align="middle">Left: hemisphere, right: importance, <code>-s 4 -l 1</code></figcaption>
</div>

<h4>
  Focus on one particular scene with at least one area light and compare the
  noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays
  (the -l flag) and with 1 sample per pixel (the -s flag) using light
  sampling, not uniform hemisphere sampling.
</h4>
<p>
  The most notable difference is the smoothness of shadow below the bunny. This
  is because
  with higher -l samples, the parts where an area light is partially occluded
  is estimated with lower noise, therefore rendering smooth shadows more
  accurately.
</p>
<div align="center">
  <table style="width=100%">
    <tr>
      <td align="middle">
        <img src="images/CBbunny-importance-s1-l1.png" width="300px" />
      </td>
      <td align="middle">
        <img src="images/CBbunny-importance-s1-l4.png" width="300px" />
      </td>
      <td align="middle">
        <img src="images/CBbunny-importance-s1-l16.png" width="300px" />
      </td>
      <td align="middle">
        <img src="images/CBbunny-importance-s1-l64.png" width="300px" />
      </td>
    </tr>
  </table>
  <figcaption align="middle">Importance sampling, <code>-s 1</code>,
    <code>-l 1,4,16,64</code></figcaption>
</div>

<h4>
  Compare the results between uniform hemisphere sampling and lighting
  sampling in a one-paragraph analysis.
</h4>
<p>
  The reason why lighting sampling results in more realistic rendering
  with much fewer samples is because it is a form of importance sampling
  that tracks the probability density of the incident radiance much better. The radiance
  along incident rays directed toward the lights are much likely to be higher
  (and therefore contributing to the actual value more),
  than for other rays that are directed elsewhere.  By focusing more on
  rays that contribute more significantly to the overall irradiance,
  lighting sampling converges to the accurate solution much faster.
</p>


<h2 align="middle">Part 4: Global Illumination</h2>

<h4>
  Walk through your implementation of the indirect lighting function.
</h4>
<p>
The <code>at_least_one_bounce_radiance</code> function follows the path tracing
pseudocode in the lecture material, where it
<li>Starts with one-bounce radiance,</li>
<li>Generates a secondary ray sample using <code>sample_f()</code>,</li>
<li>Traces the secondary ray to find the next intersection point,</li>
<li>and call itself recursively on that intersection point to add the radiance
along that secondary ray, multiplied with the BRDF of the current material
(and normalized with pdf and cpdf for unbiased estimation).</li>
</p>
<p>
In the middle of this, there are exit conditions that terminate the path by
returning from the function if
<li>the recursion depth reached <code>max_ray_depth</code>
  (which is tracked inside the ray parameter, <code>r.depth</code>), or</li>
<li>the <code>coin_flip(cpdf)</code> call returned false.
  The <code>cpdf</code> is set to 0.65 in the code.</li>
</p>

<h4>
  Show some images rendered with global (direct and indirect) illumination. Use
  1024 samples per pixel.
</h4>
<p>
</p>
<div align="center">
  <table style="width=100%">
    <tr>
      <td align="middle">
        <img src="images/CBbunny_1024_8_m5.png" width="480px" />
      </td>
      <td align="middle">
        <img src="images/CBspheres_lambertian_1024_8_m5.png" width="480px" />
      </td>
    </tr>
    <tr>
      <td align="middle">
        <img src="images/dragon_1024_8_m5.png" width="480px" />
      </td>
      <td align="middle">
        <img src="images/bunny_1024_8_m5.png" width="480px" />
      </td>
    </tr>
    <tr>
      <td align="middle">
        <img src="images/blob_1024_8_m5.png" width="480px" />
      </td>
      <td align="middle">
        <img src="images/wall-e_1024_8_m5.png" width="480px" />
      </td>
    </tr>
  </table>
  <figcaption align="middle">Global illumination render with
    <code>-s 1024 -l 8 -m 5</code></figcaption>
</div>

<h4>
  Pick one scene and compare rendered views first with only direct illumination,
  then only indirect illumination. Use 1024 samples per pixel. (You will have to
  edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate
  these views.)
</h4>
<p>
In indirect-only illumination, we can clearly see the color bleeding and
diffusive interreflections on the sides of the spheres, and on the floor
parts close to the wall.
</p>

<div align="center">
  <table style="width=100%">
    <tr>
      <td align="middle">
        <img src="images/CBspheres_lambertian-directonly.png" width="480px" />
      </td>
      <td align="middle">
        <img src="images/CBspheres_lambertian-indirectonly.png" width="480px" />
      </td>
    </tr>
  </table>
  <figcaption align="middle">Global illumination render with
    direct illumination only (0th and 1st bounce) and
    indirect illumination only (>1st bounce)</figcaption>
</div>

<h4>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3,
  and 100 (the -m flag). Use 1024 samples per pixel.
</h4>
<p>
</p>
<div align="center">
  <table style="width=100%">
    <tr>
      <td align="middle">
        <img src="images/CBbunny_m0.png" width="480px" />
      </td>
      <td align="middle">
        <img src="images/CBbunny_m1.png" width="480px" />
      </td>
      <td align="middle">
        <img src="images/CBbunny_m2.png" width="480px" />
      </td>
    </tr>
    <tr>
      <td align="middle">
        <img src="images/CBbunny_m3.png" width="480px" />
      </td>
      <td align="middle">
        <img src="images/CBbunny_m100.png" width="480px" />
      </td>
    </tr>
  </table>
  <figcaption align="middle">(Top to bottom, left to right)
    Global illumination render with <code>max_ray_depth</code> set to 0, 1, 2, 3, 100
  </figcaption>
</div>

<h4>
  Pick one scene and compare rendered views with various sample-per-pixel rates,
  including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h4>
<p>
One thing is that there is lot less noise and variance between neighboring pixels
with higher sample-per-pixel.  Also, the dark corners of the Cornell box or
under the bunny is where we can see the improvement in noisiness most clearly
with higher samples.  This is because for those dark parts, there are less incident rays
that originate from the lighting and contribute to the irradiance, and therefore need more
samples to converge to the accurate value.
</p>
<div align="center">
  <table style="width=100%">
    <tr>
      <td align="middle">
        <img src="images/CBbunny_s1.png" width="360px" />
      </td>
      <td align="middle">
        <img src="images/CBbunny_s2.png" width="360px" />
      </td>
      <td align="middle">
        <img src="images/CBbunny_s4.png" width="360px" />
      </td>
      <td align="middle">
        <img src="images/CBbunny_s8.png" width="360px" />
      </td>
    </tr>
    <tr>
      <td align="middle">
        <img src="images/CBbunny_s16.png" width="360px" />
      </td>
      <td align="middle">
        <img src="images/CBbunny_s64.png" width="360px" />
      </td>
      <td align="middle">
        <img src="images/CBbunny_s256.png" width="360px" />
      </td>
      <td align="middle">
        <img src="images/CBbunny_s1024.png" width="360px" />
      </td>
    </tr>
  </table>
  <figcaption align="middle">(Top to bottom, left to right)
    Global illumination render with <code>-s</code> set to 1, 2, 4, 8, 16, 64, 256, 1024
  </figcaption>
</div>


<h2 align="middle">Part 5: Global Illumination</h2>

<h4>
  Walk through your implementation of the adaptive sampling.
</h4>
<p>
I added an if-statement in the pixel sampling loop in
<code>PathTracer::raytrace_pixel</code> function, which gets executed
whenever the condition <code>(n_samples > 1 && n_samples % samplesPerBatch == 0)</code>
is satisfied.  In the if-statement, I compute the mean and variance value
by following the eqution in the spec, and when the I value is less or equal
to the <code>maxTolerance * mean</code>, I exit the pixel sampling loop
using a break statement.
</p>
<p>
The local variable <code>n_samples</code> keeps
track of the actual number of pixel samples taken, which may be less than
<code>ns_aa</code> if the adaptive exit condition fired.  We use this
variable to normalize the radiance sum at the end of the function, as well
as to assign to the <code>sampleCountBuffer</code> array.
</p>

<h4>
  Pick one scene and render it with at least 2048 samples per pixel. Show a good
  sampling rate image with clearly visible differences in sampling rate over
  various regions and pixels. Include both your sample rate image, which shows
  your how your adaptive sampling changes depending on which part of the image
  you are rendering, and your noise-free rendered result. Use 1 sample per light
  and at least 5 for max ray depth.
</h4>
<p>
Similar to what we discussed in Part 4, the relatively dark parts in the scene like
the corners of the CB box and the shadowed area under the spheres are what
requires the most amount of samples to converge.  This is because relatively fewer
rays originated from the light reach those surfaces, therefore requiring more
samples to get the similar number of radiance contribution from the secondary rays.
Also, the interreflections going on between the surfaces mean that the secondary
rays reach the light sources after a high number of bounces, also meaning
that more samples are required to allow such paths to actually hit the light source
and have nonzero contributions.
</p>

<div align="center">
  <table style="width=100%">
    <tr>
      <td align="middle">
        <img src="images/CBspheres_lambertian-adaptive-64-0.05.png" width="480px" />
      </td>
      <td align="middle">
        <img src="images/CBspheres_lambertian-adaptive-64-0.05_rate.png" width="480px" />
      </td>
    </tr>
  </table>
  <figcaption align="middle">
    Adaptive sampling, run with <code>-s 2048 -a 64 0.05 -l 1 -m 5</code>
  </figcaption>
</div>

</div>
</body>
</html>




